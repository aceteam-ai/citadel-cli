# services/compose/vllm.yml

services:
  vllm:
    image: vllm/vllm-openai:latest
    container_name: citadel-vllm
    ports:
      - "8000:8000"
    volumes:
      # Mount a user-accessible directory to cache Hugging Face models
      - ~/citadel-cache/huggingface:/root/.cache/huggingface
    # This command starts the server without pre-loading any specific model
    # can be started without a specific model, and it will download and
    # cache models on-demand when they are first requested via the API
    command: --host 0.0.0.0 --port 8000
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
