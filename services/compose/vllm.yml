# services/compose/vllm.yml

services:
  vllm:
    image: vllm/vllm-openai:latest
    container_name: citadel-vllm
    ports:
      - "8000:8000"
    volumes:
      # Mount a user-accessible directory to cache Hugging Face models
      - ~/citadel-cache/huggingface:/root/.cache/huggingface
    # Use an environment variable for the command.
    # The part after the colon is the default if the variable isn't set.
    command: ${VLLM_COMMAND:--host 0.0.0.0}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
