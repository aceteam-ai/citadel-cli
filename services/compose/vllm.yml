# services/compose/vllm.yml
version: "3.8"
services:
  vllm:
    # Use the official vLLM image with OpenAI-compatible endpoints
    image: vllm/vllm-openai:latest
    container_name: citadel-vllm
    ports:
      - "8000:8000"
    volumes:
      # Mount a volume to cache downloaded Hugging Face models
      - huggingface_cache:/root/.cache/huggingface
    # Command to start the server.
    # CHANGE THE MODEL to the one you want to serve from Hugging Face.
    command:
      - "--model"
      - "mistralai/Mistral-7B-v0.1"
      - "--host"
      - "0.0.0.0"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
volumes:
  huggingface_cache:
