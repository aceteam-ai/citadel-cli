# services/compose/llamacpp.yml

services:
  llamacpp:
    image: ghcr.io/ggerganov/llama.cpp:server
    container_name: citadel-llamacpp
    ports:
      - "8080:8080"
    volumes:
      # Mount a user-accessible directory for GGUF models
      - ~/citadel-cache/llamacpp:/models
    # Use an environment variable for the command.
    # The part after the colon is the default if the variable isn't set.
    command: ${LLAMACPP_COMMAND:--host 0.0.0.0 --port 8080}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
