# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Citadel CLI is an on-premise agent for the AceTeam Sovereign Compute Fabric. It connects self-hosted hardware (nodes) to the AceTeam cloud control plane, enabling users to run AI workloads (LLM inference via vLLM, Ollama, llama.cpp) on their own infrastructure while managing them through AceTeam's cloud platform.

**Key Components:**
- **Citadel**: The CLI agent that runs on user hardware
- **Nexus**: The cloud coordination server (nexus.aceteam.ai) that manages the distributed compute network
- **Node**: A physical/virtual machine running the Citadel agent
- **Services**: Dockerized AI inference engines (vLLM, Ollama, llama.cpp, LM Studio)

## Build and Development Commands

### Building
```bash
# Build for Linux (amd64 and arm64) - creates binaries in ./build/
./build.sh

# Quick local build (current architecture only)
go build -o citadel .
```

### Testing
```bash
# Run all tests
go test ./...

# Run tests with verbose output
go test -v ./...

# Run specific test
go test -v ./cmd -run TestReadManifest

# Integration tests (requires Docker)
./tests/integration.sh
```

### Running Locally
```bash
# Most commands require the citadel.yaml manifest in the current directory
# See citadel.yaml for an example configuration

# Check status
go run . status

# View node information
go run . nodes --nexus https://nexus.aceteam.ai

# Test a service
go run . test --service vllm
```

## Architecture

### Command Structure
Built with Cobra. Main command files are in `cmd/`:
- `init.go`: Provisions fresh servers (installs deps, generates config, connects to network)
- `up.go`: Brings node online, starts services, runs agent loop
- `agent.go`: Long-running job dispatcher that polls Nexus for work
- `status.go`: Health check dashboard (system vitals, GPU, network, services)
- `login.go`: Interactive Tailscale authentication
- `down.go`: Stops services defined in citadel.yaml
- `run.go`: Ad-hoc service execution without manifest
- `logs.go`: Service log streaming
- `test.go`: Service diagnostic testing

### Core Architecture Patterns

**Manifest-Driven Configuration**: The `citadel.yaml` file defines node identity and services. Generated by `citadel init`, it's the source of truth for node configuration.

**Tailscale Integration**: Network connectivity uses Tailscale for secure mesh networking between nodes and Nexus. Commands check Tailscale status via `tailscale status --json` and authenticate using either browser flow or authkeys.

**Job Handler Pattern**: The agent uses a pluggable handler system for remote job execution:
```go
type JobHandler interface {
    Execute(ctx JobContext, job *nexus.Job) (output []byte, err error)
}
```
Handlers in `internal/jobs/` implement specific job types (shell commands, model downloads, inference requests). The agent polls Nexus, dispatches to appropriate handler, and reports status back.

**Embedded Services**: Docker Compose files for services are embedded in the binary at `services/compose/*.yml` using Go's `embed` package. The `services.ServiceMap` provides lookup by name (vllm, ollama, llamacpp, lmstudio).

**Docker Compose Management**: Services are managed through `docker compose` commands. The code uses subprocess calls to docker/docker-compose CLI for container lifecycle.

### Key Packages

- **`cmd/`**: Cobra command implementations
- **`internal/nexus/`**: HTTP client for Nexus API, network helpers for Tailscale operations
- **`internal/jobs/`**: Job handler implementations (shell, inference, model download)
- **`internal/ui/`**: Interactive prompts using survey library
- **`services/`**: Embedded Docker Compose files and service registry

### Network Architecture

Citadel uses Tailscale to create a secure mesh network:
1. Node authenticates to Tailscale network using authkey or browser OAuth
2. Joins AceTeam's Tailnet with hostname matching node name from citadel.yaml
3. Communicates with Nexus over Tailscale IPs
4. Agent polls Nexus API for jobs to execute

### Provisioning Flow (`citadel init`)

1. **Network Choice**: Checks if already connected to Tailscale, prompts for authkey/browser/skip
2. **Service Selection**: Interactive prompt or `--service` flag to choose inference engine
3. **Node Naming**: Prompts for node name or uses `--node-name` flag
4. **System Provisioning**: Sequential installation of dependencies:
   - Updates apt packages
   - Installs curl, wget, ca-certificates, gnupg, lsb-release
   - Installs Docker (using official Docker install script if needed)
   - Creates `citadel` system user and adds to docker group
   - Installs NVIDIA Container Toolkit (warns if fails on non-GPU systems)
   - Configures Docker daemon for NVIDIA runtime
   - Installs Tailscale
5. **Config Generation**: Creates `~/citadel-node/` directory with:
   - `citadel.yaml` manifest
   - `services/*.yml` Docker Compose files
   - Copies citadel binary to config directory
6. **Network Connection**: Joins Tailscale network if authkey provided
7. **Service Startup**: Runs `citadel up` to start configured services

### Agent Loop

The agent (`citadel up` or `citadel agent`) runs continuously:
1. Polls Nexus `/api/v1/jobs/next` every 5 seconds
2. Receives job with `{id, type, payload}` structure
3. Dispatches to registered handler based on job type
4. Executes handler and captures output
5. Reports status back to Nexus with `{status: "SUCCESS"|"FAILURE", output: "..."}`

Job handlers registered in `cmd/agent.go:init()` map job types to handler implementations.

## Important Implementation Notes

### Manifest Loading
`citadel.yaml` location discovery (in `cmd/up.go:findAndReadManifest()`):
1. Checks current directory
2. Checks `/etc/citadel/citadel.yaml` (global system config)
3. Falls back to `~/citadel-node/citadel.yaml`

### GPU Detection
Status command detects NVIDIA GPUs using:
1. `nvidia-smi` command output parsing
2. Falls back to checking `/proc/driver/nvidia/gpus/` directory
3. Displays "No GPU detected" if neither method succeeds

### Service Management
Services are started with `docker compose -f <path> -p citadel-<name> up -d`. The `-p` flag ensures consistent naming: `citadel-vllm`, `citadel-ollama`, etc.

### Docker Runtime Requirements
vLLM and llama.cpp require NVIDIA runtime configured in `/etc/docker/daemon.json`:
```json
{
  "default-runtime": "nvidia",
  "runtimes": {
    "nvidia": {
      "path": "nvidia-container-runtime",
      "runtimeArgs": []
    }
  }
}
```
The `init` command configures this automatically.

### Authentication Patterns
Three auth flows supported:
1. **Device Authorization** (RFC 8628): OAuth 2.0 device flow with code display (Claude Code-style)
   - User runs `citadel init` → CLI displays device code → User enters code at aceteam.ai/device
   - Implemented in `internal/nexus/deviceauth.go` and `internal/ui/devicecode.go`
   - Default/recommended flow for interactive use
2. **Authkey**: Non-interactive, uses pre-generated single-use keys from Nexus admin panel
   - Still supported via `--authkey` flag for automation/CI/CD
3. **Browser**: Legacy interactive OAuth flow using `tailscale login`

The device flow polls `/api/fabric/device-auth/token` endpoint until user approves at aceteam.ai/device.

**Configuration:**
- `--auth-service <url>` flag or `CITADEL_AUTH_HOST` env var sets auth service URL (default: https://aceteam.ai)
- `--nexus <url>` flag sets Headscale server URL (default: https://nexus.aceteam.ai)

## Testing Philosophy

Integration tests in `tests/integration.sh` use `docker-compose.test.yml` to spin up a mock Nexus server and test the full agent lifecycle.

Unit tests focus on manifest parsing and utility functions. Most command logic is tested through integration tests since it requires Docker/Tailscale.

## Common Gotchas

**Sudo Requirements**: `citadel init` and `citadel login` require sudo for system provisioning and Tailscale authentication.

**Docker Group Membership**: After `init`, users must log out and back in (or run `exec su -l $USER`) for Docker group membership to take effect.

**Compose File Paths**: Service compose files in citadel.yaml use relative paths from the manifest location, not from the current working directory.

**Version Injection**: The `build.sh` script injects version via linker flags: `-ldflags="-X '${MODULE_PATH}/cmd.Version=${VERSION}'"`. Version is set as global var in `cmd/version.go`.

**Mock Mode**:
- The Nexus client in `internal/nexus/client.go` has a mock mode using `mock_jobs.json` for local testing
- Device auth client has mock server in `internal/nexus/deviceauth_mock.go` for testing without backend
  - Usage: `mock := nexus.StartMockDeviceAuthServer(3); defer mock.Close()`
  - Returns `authorization_pending` for N polls, then returns success

## macOS Support

Citadel CLI has full cross-platform support for both Linux and macOS (darwin). The codebase uses platform abstraction layers in `internal/platform/` to handle OS-specific operations.

### Platform Abstractions

**Core Platform Utilities** (`internal/platform/platform.go`):
- `IsLinux()`, `IsDarwin()` - OS detection
- `IsRoot()` - Privilege checking (works on both Linux and macOS)
- `HomeDir(username)` - Cross-platform home directory resolution
- `ConfigDir()` - Returns `/etc/citadel` on Linux, `/usr/local/etc/citadel` on macOS

**Package Management** (`internal/platform/packages.go`):
- `GetPackageManager()` - Returns apt (Linux) or brew (macOS) manager
- `EnsureHomebrew()` - Installs Homebrew if not present on macOS

**User Management** (`internal/platform/users.go`):
- `GetUserManager()` - Returns Linux (useradd/usermod) or Darwin (dscl) manager
- Handles user and group creation across platforms

**Docker Management** (`internal/platform/docker.go`):
- `GetDockerManager()` - Returns Docker Engine (Linux) or Docker Desktop (macOS) manager
- Handles installation, startup, and permissions appropriately per platform

**GPU Detection** (`internal/platform/gpu.go`):
- `GetGPUDetector()` - Returns NVIDIA (Linux) or Metal (macOS) detector
- Linux: Uses `nvidia-smi` and `lspci` for NVIDIA GPU detection
- macOS: Uses `system_profiler SPDisplaysDataType` for Metal-compatible GPU detection

### Platform-Specific Behavior

**Linux**:
- Uses apt package manager for dependencies
- Installs Docker Engine via official script
- Configures NVIDIA Container Toolkit for GPU support
- Uses systemctl for service management
- Creates system users with useradd/usermod

**macOS**:
- Uses Homebrew for package management (auto-installs if missing)
- Installs Docker Desktop via `brew install --cask docker`
- GPU support handled automatically by Docker Desktop (especially on Apple Silicon)
- No NVIDIA Container Toolkit (not applicable)
- Creates users with dscl (Directory Service command line)
- Uses `/usr/local/etc/citadel` for global config instead of `/etc/citadel`

### GPU Support Notes

**Linux**: Full NVIDIA GPU support via NVIDIA Container Toolkit. Compose files use `driver: nvidia` specification.

**macOS**:
- Docker Desktop on Apple Silicon (M1/M2/M3) has built-in GPU support via Metal framework
- Intel Macs do not have GPU acceleration for containers
- The `driver: nvidia` specifications in compose files are Linux-specific and ignored on macOS
- Services will still run on macOS but without explicit GPU device reservations
- Docker Desktop automatically handles GPU access for Metal-compatible workloads

### Known Limitations on macOS

- NVIDIA Container Toolkit steps are skipped (not applicable)
- systemctl commands are not used (Docker Desktop manages the daemon)
- User/group management uses different commands (dscl vs useradd)
- Passwordless sudo configuration only applies to Linux
- GPU device reservations in compose files are Linux-specific
