# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Citadel CLI is an on-premise agent for the AceTeam Sovereign Compute Fabric. It connects self-hosted hardware (nodes) to the AceTeam cloud control plane, enabling users to run AI workloads (LLM inference via vLLM, Ollama, llama.cpp) on their own infrastructure while managing them through AceTeam's cloud platform.

**Key Components:**
- **Citadel**: The CLI agent that runs on user hardware
- **Nexus**: The cloud coordination server (nexus.aceteam.ai) that manages the distributed compute network
- **Node**: A physical/virtual machine running the Citadel agent
- **Services**: Dockerized AI inference engines (vLLM, Ollama, llama.cpp, LM Studio)

## Build and Development Commands

### Building
```bash
# Build for Linux (amd64 and arm64) - creates binaries in ./build/
./build.sh

# Quick local build (current architecture only)
go build -o citadel .
```

### Testing
```bash
# Run all tests
go test ./...

# Run tests with verbose output
go test -v ./...

# Run specific test
go test -v ./cmd -run TestReadManifest

# Integration tests (requires Docker)
./tests/integration.sh
```

### Running Locally
```bash
# Most commands require the citadel.yaml manifest in the current directory
# See citadel.yaml for an example configuration

# Check status
go run . status

# View node information
go run . nodes --nexus https://nexus.aceteam.ai

# Test a service
go run . test --service vllm
```

## Architecture

### Command Structure
Built with Cobra. Main command files are in `cmd/`:
- `init.go`: Provisions fresh servers (installs deps, generates config, connects to network)
- `up.go`: Brings node online, starts services, runs agent loop
- `agent.go`: Long-running job dispatcher that polls Nexus for work
- `status.go`: Health check dashboard (system vitals, GPU, network, services)
- `login.go`: Interactive Tailscale authentication
- `down.go`: Stops services defined in citadel.yaml
- `run.go`: Ad-hoc service execution without manifest
- `logs.go`: Service log streaming
- `test.go`: Service diagnostic testing

### Core Architecture Patterns

**Manifest-Driven Configuration**: The `citadel.yaml` file defines node identity and services. Generated by `citadel init`, it's the source of truth for node configuration.

**Tailscale Integration**: Network connectivity uses Tailscale for secure mesh networking between nodes and Nexus. Commands check Tailscale status via `tailscale status --json` and authenticate using either browser flow or authkeys.

**Job Handler Pattern**: The agent uses a pluggable handler system for remote job execution:
```go
type JobHandler interface {
    Execute(ctx JobContext, job *nexus.Job) (output []byte, err error)
}
```
Handlers in `internal/jobs/` implement specific job types (shell commands, model downloads, inference requests). The agent polls Nexus, dispatches to appropriate handler, and reports status back.

**Embedded Services**: Docker Compose files for services are embedded in the binary at `services/compose/*.yml` using Go's `embed` package. The `services.ServiceMap` provides lookup by name (vllm, ollama, llamacpp, lmstudio).

**Docker Compose Management**: Services are managed through `docker compose` commands. The code uses subprocess calls to docker/docker-compose CLI for container lifecycle.

### Key Packages

- **`cmd/`**: Cobra command implementations
- **`internal/nexus/`**: HTTP client for Nexus API, network helpers for Tailscale operations
- **`internal/jobs/`**: Job handler implementations (shell, inference, model download)
- **`internal/ui/`**: Interactive prompts using survey library
- **`services/`**: Embedded Docker Compose files and service registry

### Network Architecture

Citadel uses Tailscale to create a secure mesh network:
1. Node authenticates to Tailscale network using authkey or browser OAuth
2. Joins AceTeam's Tailnet with hostname matching node name from citadel.yaml
3. Communicates with Nexus over Tailscale IPs
4. Agent polls Nexus API for jobs to execute

### Provisioning Flow (`citadel init`)

1. **Network Choice**: Checks if already connected to Tailscale, prompts for authkey/browser/skip
2. **Service Selection**: Interactive prompt or `--service` flag to choose inference engine
3. **Node Naming**: Prompts for node name or uses `--node-name` flag
4. **System Provisioning**: Sequential installation of dependencies:
   - Updates apt packages
   - Installs curl, wget, ca-certificates, gnupg, lsb-release
   - Installs Docker (using official Docker install script if needed)
   - Creates `citadel` system user and adds to docker group
   - Installs NVIDIA Container Toolkit (warns if fails on non-GPU systems)
   - Configures Docker daemon for NVIDIA runtime
   - Installs Tailscale
5. **Config Generation**: Creates `~/citadel-node/` directory with:
   - `citadel.yaml` manifest
   - `services/*.yml` Docker Compose files
   - Copies citadel binary to config directory
6. **Network Connection**: Joins Tailscale network if authkey provided
7. **Service Startup**: Runs `citadel up` to start configured services

### Agent Loop

The agent (`citadel up` or `citadel agent`) runs continuously:
1. Polls Nexus `/api/v1/jobs/next` every 5 seconds
2. Receives job with `{id, type, payload}` structure
3. Dispatches to registered handler based on job type
4. Executes handler and captures output
5. Reports status back to Nexus with `{status: "SUCCESS"|"FAILURE", output: "..."}`

Job handlers registered in `cmd/agent.go:init()` map job types to handler implementations.

## Important Implementation Notes

### Manifest Loading
`citadel.yaml` location discovery (in `cmd/up.go:findAndReadManifest()`):
1. Checks current directory
2. Checks `/etc/citadel/citadel.yaml` (global system config)
3. Falls back to `~/citadel-node/citadel.yaml`

### GPU Detection
Status command detects NVIDIA GPUs using:
1. `nvidia-smi` command output parsing
2. Falls back to checking `/proc/driver/nvidia/gpus/` directory
3. Displays "No GPU detected" if neither method succeeds

### Service Management
Services are started with `docker compose -f <path> -p citadel-<name> up -d`. The `-p` flag ensures consistent naming: `citadel-vllm`, `citadel-ollama`, etc.

### Docker Runtime Requirements
vLLM and llama.cpp require NVIDIA runtime configured in `/etc/docker/daemon.json`:
```json
{
  "default-runtime": "nvidia",
  "runtimes": {
    "nvidia": {
      "path": "nvidia-container-runtime",
      "runtimeArgs": []
    }
  }
}
```
The `init` command configures this automatically.

### Authentication Patterns
Two auth flows supported:
1. **Authkey**: Non-interactive, uses pre-generated single-use keys from Nexus admin panel
2. **Browser**: Interactive OAuth flow using `tailscale login`

The `--authkey` flag enables automated provisioning for CI/CD and fleet management.

## Testing Philosophy

Integration tests in `tests/integration.sh` use `docker-compose.test.yml` to spin up a mock Nexus server and test the full agent lifecycle.

Unit tests focus on manifest parsing and utility functions. Most command logic is tested through integration tests since it requires Docker/Tailscale.

## Common Gotchas

**Sudo Requirements**: `citadel init` and `citadel login` require sudo for system provisioning and Tailscale authentication.

**Docker Group Membership**: After `init`, users must log out and back in (or run `exec su -l $USER`) for Docker group membership to take effect.

**Compose File Paths**: Service compose files in citadel.yaml use relative paths from the manifest location, not from the current working directory.

**Version Injection**: The `build.sh` script injects version via linker flags: `-ldflags="-X '${MODULE_PATH}/cmd.Version=${VERSION}'"`. Version is set as global var in `cmd/version.go`.

**Mock Mode**: The Nexus client in `internal/nexus/client.go` has a mock mode using `mock_jobs.json` for local testing without a live Nexus server.
